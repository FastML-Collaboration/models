{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd3c1658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home1/giorgian/projects/trigger-detection-pipeline/sPHENIX/tracking-GNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/giorgian/anaconda3/envs/jupyter/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32d4c8fa-7a57-4e7a-805d-0310fb0869fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import replace\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import sys\n",
    "import logging\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from models.bgn_st_tracking import GNNSegmentClassifier\n",
    "from icecream import ic\n",
    "from numpy.linalg import inv\n",
    "import sklearn.metrics as metrics\n",
    "from datasets import get_data_loaders\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cbdfbf66-78e0-4bbf-b5bf-c1e69d8873c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_track_endpoints(hits, good_layers):\n",
    "    # Assumption: all tracks have at least 1 hit\n",
    "    # If it has one hit, first_hit == last_hit for that track\n",
    "    # hits shape: (n_tracks, 5, 3)\n",
    "    # good_layers shape: (n_tracks, 5)\n",
    "    min_indices = good_layers * np.arange(5) + (1 - good_layers) * np.arange(5, 10)\n",
    "    indices = np.expand_dims(np.argmin(min_indices, axis=-1), -1)\n",
    "    indices = np.expand_dims(indices, axis=-2)\n",
    "    first_hits = np.take_along_axis(hits, indices, axis=-2)\n",
    "    max_indices = good_layers * np.arange(5, 10) + (1 - good_layers) * np.arange(5)\n",
    "    indices = np.expand_dims(np.argmax(max_indices, axis=-1), -1)\n",
    "    indices = np.expand_dims(indices, axis=-2)\n",
    "    last_hits = np.take_along_axis(hits, indices, axis=-2)\n",
    "    return first_hits.squeeze(1), last_hits.squeeze(1)\n",
    "\n",
    "def get_predicted_pz(track_hits, good_layers, radius):\n",
    "    hits = track_hits.reshape(-1, 5, 3)\n",
    "    first_hit, last_hit = get_track_endpoints(hits, good_layers)\n",
    "    dz = (last_hit[:, -1] - first_hit[:, -1])/100\n",
    "    chord2 = ((last_hit[:, 0] - first_hit[:, 0]) ** 2 + (last_hit[:, 1] - first_hit[:, 1]) ** 2) / 10000\n",
    "    r2 = 2*radius**2\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        dtheta = np.arccos((r2 - chord2) / (r2 + (r2 == 0)))\n",
    "    dtheta += (dtheta == 0)\n",
    "    return np.nan_to_num(dz / dtheta)\n",
    "\n",
    "def matmul_3D(A, B):\n",
    "    return np.einsum('lij,ljk->lik', A, B)\n",
    "\n",
    "\n",
    "def get_approximate_radii(track_hits, good_layers, n_layers):\n",
    "    x_indices = [3*j for j in range(5)]\n",
    "    y_indices = [3*j+1 for j in range(5)]\n",
    "    r = np.zeros(track_hits.shape[0])\n",
    "    centers = np.zeros((track_hits.shape[0], 2))\n",
    "    for n_layer in range(3, 5 + 1):\n",
    "        complete_tracks = track_hits[n_layers == n_layer]\n",
    "        hit_indices = good_layers[n_layers == n_layer]\n",
    "        if complete_tracks.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        A = np.ones((complete_tracks.shape[0], n_layer, 3))\n",
    "        x_values = complete_tracks[:, x_indices]\n",
    "        x_values = x_values[hit_indices].reshape(complete_tracks.shape[0], n_layer)\n",
    "\n",
    "        y_values = complete_tracks[:, y_indices]\n",
    "        y_values = y_values[hit_indices].reshape(complete_tracks.shape[0], n_layer)\n",
    "        A[:, :, 0] = x_values\n",
    "        A[:, :, 1] = y_values\n",
    "\n",
    "        y = - x_values**2 - y_values**2\n",
    "        y = y.reshape((y.shape[0], y.shape[1], 1))\n",
    "        AT = np.transpose(A, axes=(0, 2, 1))\n",
    "        c = matmul_3D(matmul_3D(inv(matmul_3D(AT, A)), AT), y)[..., 0]\n",
    "        r[n_layers == n_layer] = np.sqrt(c[:, 0]**2 + c[:, 1]**2 - 4*c[:, 2])/200\n",
    "        centers[n_layers == n_layer] = np.stack([-c[:, 0]/2, -c[:, 1]/2], axis=-1)\n",
    "\n",
    "    #test = get_approximate_radius(track_hits, n_layers == 5)\n",
    "    #assert np.allclose(test, r[n_layers == 5])\n",
    "\n",
    "    return r, centers\n",
    "\n",
    "def get_length(start, end):\n",
    "    return np.sqrt(np.sum((start - end)**2, axis=1))\n",
    "\n",
    "\n",
    "def port_event(batch, batch_output, ip_output, trigger_output, output_file):\n",
    "    \"\"\"\n",
    "    Ported function to construct track_hits from batch.x_intt, batch.x_mvtx, and batch.edge_index.\n",
    "    \n",
    "    Parameters:\n",
    "      batch: an object with attributes:\n",
    "            - x_intt: shape (N, 10) [each row has two hits, each hit with (r, phi, z, layer_id, n_pixels)]\n",
    "            - x_mvtx: shape (M, 5) [each row is (r, phi, z, layer_id, n_pixels)]\n",
    "            - edge_index: shape (2, num_edges) linking x_intt indices (first row) to x_mvtx indices (second row)\n",
    "            - interaction_point: an iterable giving the true interaction point (e.g. [x, y, z])\n",
    "            - trigger: a boolean flag for trigger\n",
    "      batch_output: predictions for the edges (use batch_output > 0 to decide if an edge is “true”)\n",
    "      ip_output: model’s predicted interaction point (to be saved as interaction_point_pred)\n",
    "      trigger_output: model’s predicted trigger output (to be saved as trigger_pred)\n",
    "      output_file: destination path for the npz output file.\n",
    "    \"\"\"\n",
    "    # Mapping from raw layer_id to track layer (0-indexed):\n",
    "    # 0 -> 0, 1 -> 1, 2 -> 2, 3 -> 3, 4 -> 3, 5 -> 4, 6 -> 4.\n",
    "    layer_map = {0: 0, 1: 1, 2: 2, 3: 3, 4: 3, 5: 4, 6: 4}\n",
    "    \n",
    "    x_intt = batch.x_intt.detach().cpu().numpy()   # shape (N, 10)\n",
    "    x_mvtx = batch.x_mvtx.detach().cpu().numpy()    # shape (M, 5)\n",
    "    edge_index = batch.edge_index  # shape (2, num_edges)\n",
    "    \n",
    "    num_tracks = x_intt.shape[0]\n",
    "    # Prepare an array to hold the track vectors: 5 layers x 3 coordinates = 15 per track.\n",
    "    track_hits = np.zeros((num_tracks, 15), dtype=np.float32)\n",
    "    \n",
    "    # Create a mapping from each x_intt hit (track candidate) to the list of associated x_mvtx indices.\n",
    "    # We assume edge_index[0] contains indices into x_intt and edge_index[1] indices into x_mvtx.\n",
    "    track_to_mvtx = {i: [] for i in range(num_tracks)}\n",
    "    \n",
    "    # Filter edge_index based on the mask (true edges: batch_output > 0)\n",
    "    true_mask = batch_output > 0\n",
    "    true_edges = edge_index[:, true_mask]  # shape (2, num_true_edges)\n",
    "    \n",
    "    # Group associated mvtx indices by their corresponding intt index.\n",
    "    for intt_idx, mvtx_idx in zip(true_edges[0], true_edges[1]):\n",
    "        intt_idx = int(intt_idx)\n",
    "        mvtx_idx = int(mvtx_idx)\n",
    "        track_to_mvtx[intt_idx].append(mvtx_idx)\n",
    "    \n",
    "    # Process each track (each row in x_intt)\n",
    "    for i in range(num_tracks):\n",
    "        # Dictionary to collect (x,y,z) hits for each track layer (0 through 4).\n",
    "        hits_per_layer = {layer: [] for layer in range(5)}\n",
    "        \n",
    "        # --- Process the two hits from x_intt ---\n",
    "        # First hit: entries 0 to 4.\n",
    "        hit1 = x_intt[i,  [0, 1, 2, 6, 8]]  # (r, phi, z, layer_id, n_pixels)\n",
    "        r, phi, z, _, layer_id = hit1\n",
    "        r *= 3\n",
    "        z *= 3\n",
    "        x_coord = r * np.cos(phi)\n",
    "        y_coord = r * np.sin(phi)\n",
    "        mapped_layer = layer_map[int(layer_id)]\n",
    "        hits_per_layer[mapped_layer].append([x_coord, y_coord, z])\n",
    "        \n",
    "        # Second hit: entries 5 to 10.\n",
    "        hit2 = x_intt[i, [3, 4, 5, 7, 9]]\n",
    "        r, phi, z, _, layer_id = hit2\n",
    "        r *= 3\n",
    "        z *= 3\n",
    "        x_coord = r * np.cos(phi)\n",
    "        y_coord = r * np.sin(phi)\n",
    "        mapped_layer = layer_map[int(layer_id)]\n",
    "        hits_per_layer[mapped_layer].append([x_coord, y_coord, z])\n",
    "        \n",
    "        # --- Process associated x_mvtx hits ---\n",
    "        for mvtx_idx in track_to_mvtx[i]:\n",
    "            hit = x_mvtx[mvtx_idx]  # shape (5,)\n",
    "            r, phi, z, _, layer_id = hit\n",
    "            r *= 3\n",
    "            z *= 3\n",
    "            x_coord = r * np.cos(phi)\n",
    "            y_coord = r * np.sin(phi)\n",
    "            mapped_layer = layer_map[int(layer_id)]\n",
    "            hits_per_layer[mapped_layer].append([x_coord, y_coord, z])\n",
    "        \n",
    "        # --- Average hits per layer ---\n",
    "        # The track vector is arranged as:\n",
    "        # [layer0_x, layer0_y, layer0_z, layer1_x, layer1_y, layer1_z, ..., layer4_x, layer4_y, layer4_z]\n",
    "        for layer in range(5):\n",
    "            if hits_per_layer[layer]:\n",
    "                # Compute the mean coordinate for this layer.\n",
    "                avg_coord = np.mean(np.array(hits_per_layer[layer]), axis=0)\n",
    "                track_hits[i, 3*layer:3*layer+3] = avg_coord\n",
    "            # If no hits exist for a layer, the corresponding entries remain 0.\n",
    "    \n",
    "    # --- Compute helper variables ---\n",
    "    # Reshape to (num_tracks, 5, 3) and create a boolean mask indicating which layers have at least one hit.\n",
    "    good_layers = np.any(track_hits.reshape(num_tracks, 5, 3) != 0, axis=-1)\n",
    "    n_layers = np.sum(good_layers, axis=-1)\n",
    "    \n",
    "    # Use batch.interaction_point (the true collision vertex) for output.\n",
    "    ip = tuple(float(x) for x in batch.interaction_point[0])\n",
    "    \n",
    "    # Call the helper functions (assumed to be implemented elsewhere)\n",
    "    radii, centers = get_approximate_radii(track_hits, good_layers, n_layers)\n",
    "    p_z = get_predicted_pz(track_hits, good_layers, radii)\n",
    "    \n",
    "    # --- Save output ---\n",
    "    # Save the numpy file with the required keys. Note that we are not shuffling the tracks.\n",
    "    np.savez(\n",
    "        output_file,\n",
    "        collision_vertex=ip,\n",
    "        tracks=track_hits,\n",
    "        radii=radii,\n",
    "        p_z=p_z,\n",
    "        centers=centers,\n",
    "        trigger=batch.trigger.cpu().numpy()[0],\n",
    "        trigger_pred=trigger_output.detach().cpu().numpy(),\n",
    "        interaction_point_pred=ip_output.detach().cpu().numpy()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4793f603-834a-4df2-8220-eabeae3ce8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 7.1390e+00,  1.2387e+00, -1.0972e+01,  1.0000e+00,  3.0000e+00],\n",
       "         [ 7.5817e+00, -2.9723e+00, -1.0972e+01,  2.0000e+00,  3.0000e+00],\n",
       "         [ 7.5708e+00, -2.9554e+00, -1.0972e+01,  1.0000e+00,  3.0000e+00],\n",
       "         [ 7.6388e+00, -1.8861e-01,  6.8275e+00,  1.0000e+00,  3.0000e+00],\n",
       "         [ 7.6416e+00, -1.8352e-01,  6.8275e+00,  7.0000e+00,  3.0000e+00],\n",
       "         [ 7.4943e+00,  3.4347e-01,  1.0028e+01,  1.0000e+00,  3.0000e+00],\n",
       "         [ 7.6591e+00, -2.3313e+00,  1.1628e+01,  1.0000e+00,  3.0000e+00],\n",
       "         [ 7.5010e+00,  1.7474e-01,  1.3628e+01,  1.0000e+00,  3.0000e+00],\n",
       "         [ 7.4757e+00,  3.0029e-01,  1.5628e+01,  2.0000e+00,  3.0000e+00],\n",
       "         [ 8.1788e+00, -2.9820e+00, -1.4572e+01,  1.0000e+00,  4.0000e+00],\n",
       "         [ 8.1550e+00, -1.1833e-02,  4.2755e-01,  5.0000e+00,  4.0000e+00],\n",
       "         [ 8.1824e+00, -8.0037e-02,  6.8275e+00,  6.0000e+00,  4.0000e+00],\n",
       "         [ 8.2764e+00, -2.0345e+00,  1.5628e+01,  2.0000e+00,  4.0000e+00],\n",
       "         [ 8.2953e+00, -1.7106e+00,  1.3628e+01,  5.0000e+00,  4.0000e+00],\n",
       "         [ 7.7157e+00, -1.7205e+00,  1.9628e+01,  4.0000e+00,  3.0000e+00],\n",
       "         [ 7.8056e+00,  1.0974e+00,  1.9628e+01,  1.0000e+00,  4.0000e+00],\n",
       "         [ 7.9898e+00,  5.7187e-01,  1.9628e+01,  2.0000e+00,  4.0000e+00]],\n",
       "        device='cuda:0'),\n",
       " tensor([[10.5801,  0.3297, -2.9725,  2.0000,  6.0000],\n",
       "         [10.7862, -2.3153,  2.0275,  1.0000,  6.0000],\n",
       "         [10.7862, -2.3153,  2.0275,  1.0000,  6.0000],\n",
       "         [ 9.9804,  0.1631,  5.2275,  8.0000,  5.0000],\n",
       "         [ 9.9804,  0.1631,  5.2275,  8.0000,  5.0000],\n",
       "         [ 9.9229,  0.4607,  5.2275,  1.0000,  5.0000],\n",
       "         [10.1809, -2.0789, 15.6275,  1.0000,  5.0000],\n",
       "         [10.5601,  0.3832, 17.6276,  1.0000,  6.0000],\n",
       "         [10.5601,  0.3832, 17.6276,  1.0000,  6.0000],\n",
       "         [10.7862, -2.3153,  2.0275,  1.0000,  6.0000],\n",
       "         [10.5801,  0.3297, -2.9725,  2.0000,  6.0000],\n",
       "         [ 9.9804,  0.1631,  5.2275,  8.0000,  5.0000],\n",
       "         [10.1809, -2.0789, 15.6275,  1.0000,  5.0000],\n",
       "         [10.7969, -1.9925, 15.6275,  5.0000,  6.0000],\n",
       "         [10.7969, -1.9925, 15.6275,  5.0000,  6.0000],\n",
       "         [10.5601,  0.3832, 17.6276,  1.0000,  6.0000],\n",
       "         [10.5601,  0.3832, 17.6276,  1.0000,  6.0000]], device='cuda:0'))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.x_intt[:, [0, 1, 2, 6, 8]],batch.x_intt[:, [3, 4, 5, 7, 9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1a9e077",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eb5ddd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tracking_results/agnn/agnn-lr0.0008127498598898657-b24-d64-PReLU-gi1-ln-False-n50000/experiment_2025-03-25_09:52:45/checkpoints/model_checkpoint_017.pth.tar\n",
      "Successfully reloaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1104680/4258571413.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_file, map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "# create model and load checkpoint\n",
    "model_result_folder = '../tracking_results/agnn/agnn-lr0.0008127498598898657-b24-d64-PReLU-gi1-ln-False-n50000/experiment_2025-03-25_09:52:45/'\n",
    "config_file = model_result_folder + '/config.pkl'\n",
    "config = pickle.load(open(config_file, 'rb'))\n",
    "data_config = config.get('data')\n",
    "\n",
    "model_config = config.get('model', {})\n",
    "model_config.pop('loss_func')\n",
    "model_config.pop('name')\n",
    "model = GNNSegmentClassifier(**model_config).to(DEVICE)\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer=None):\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    if optimizer != None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        return model, optimizer\n",
    "    return model\n",
    "\n",
    "# load_checkpoint\n",
    "checkpoint_dir = os.path.join(model_result_folder, 'checkpoints')\n",
    "checkpoint_file = sorted([os.path.join(checkpoint_dir, f) for f in os.listdir(checkpoint_dir) if f.startswith('model_checkpoint')])\n",
    "checkpoint_file = checkpoint_file[-1]\n",
    "print(checkpoint_file)\n",
    "model = load_checkpoint(checkpoint_file, model)\n",
    "print('Successfully reloaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e059db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config['batch_size'] = 1\n",
    "data_config['n_train'] = 1\n",
    "data_config['n_valid'] = 34000\n",
    "#data_config['input_dir2'] = '/ssd1/giorgian/hits-data-august-2022-ctypes/trigger/1'\n",
    "#data_config['force_inputdir2_nontrigger'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c13f7fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader, valid_data_loader = get_data_loaders(distributed=False, rank=0, n_ranks=0, **data_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0db2a65c-0b24-4937-84f0-a9dd9e9a4a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_output_dir = '/disks/disk1/giorgian/bbar-data-march-2025/trigger/'\n",
    "nontrigger_output_dir = '/disks/disk1/giorgian/bbar-data-march-2025/nontrigger/'\n",
    "output_dirs = (trigger_output_dir, nontrigger_output_dir)\n",
    "\n",
    "for output_dir in output_dirs:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "13152db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f365ef705c784ec18d5ec473f4c18c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batch in tqdm(valid_data_loader):\n",
    "    # Run the model to get predictions\n",
    "    batch = batch.to(DEVICE)\n",
    "    batch_output, ip_output, trigger_output = model(batch)\n",
    "\n",
    "    # Determine the output filename and directory based on batch.filename[0] and batch.trigger[0].\n",
    "    fname = batch.filename[0]\n",
    "    if batch.trigger[0]:\n",
    "        output_file = os.path.join(trigger_output_dir, os.path.basename(fname))\n",
    "    else:\n",
    "        output_file = os.path.join(nontrigger_output_dir, os.path.basename(fname))\n",
    "\n",
    "    # Process the batch and save the output.\n",
    "    port_event(batch, batch_output, ip_output, trigger_output, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "15c9c7c9-2ab5-4927-aa25-238383431211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5534e-02, 1.2005e-03, 4.0199e+01]], device='cuda:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.interaction_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bae1fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98650014-fd49-4103-86a4-79468a126aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
