{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.stats import mode\n",
    "from icecream import ic\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.cm as cm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = os.path.abspath('../')\n",
    "sys.path.append(project_dir)\n",
    "os.chdir(project_dir)\n",
    "\n",
    "from utils.log import load_config\n",
    "from utils.log import load_checkpoint\n",
    "from models.Bipartite_Attention_Masked import Bipartite_Attention as Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home1/tingtingxuan/physics-trigger-graph-level-prediction/train_results/ecml-masked-rerun/experiment_2022-08-24_10:57:34/checkpoints/model_checkpoint_018.pth.tar\n",
      "Successfully reloaded!\n"
     ]
    }
   ],
   "source": [
    "result_dir = '/home1/tingtingxuan/physics-trigger-graph-level-prediction/train_results/ecml-masked-rerun/experiment_2022-08-24_10:57:34'\n",
    "config_file = os.path.join(result_dir, 'config.pkl')\n",
    "config = pickle.load(open(config_file, 'rb'))\n",
    "\n",
    "mconfig = config['model']\n",
    "mconfig['num_features'] += 13*config['data']['add_geo_features'] + config['data']['use_radius']\n",
    "model = Model(**mconfig)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "checkpoint_dir = os.path.join(result_dir, 'checkpoints')\n",
    "checkpoint_file = sorted([os.path.join(checkpoint_dir, f) for f in os.listdir(checkpoint_dir) if f.startswith('model_checkpoint')])\n",
    "checkpoint_file = checkpoint_file[17]\n",
    "print(checkpoint_file)\n",
    "model = load_checkpoint(checkpoint_file, model)\n",
    "print('Successfully reloaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'trkvec-ecml-masked-rerun', 'input_dir': '/ssd2/tingting/HFMLNewFiles-old-parsed/trigger/1', 'input_dir2': '/ssd2/tingting/HFMLNewFiles-old-parsed/nontrigger/0', 'n_train1': 500000, 'n_valid1': 200000, 'n_test1': 200000, 'n_train2': 500000, 'n_valid2': 200000, 'n_test2': 200000, 'add_geo_features': True, 'use_radius': True, 'batch_size': 32, 'n_workers': 4, 'load_complete_graph': False}\n"
     ]
    }
   ],
   "source": [
    "print(config['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_samples1 = 500 #background\n",
    "# test_samples2 = 500 #trigger\n",
    "test_samples1 = 99900 \n",
    "test_samples2 = 100\n",
    "batch_size = 32\n",
    "data_mode = 'gt_track'\n",
    "\n",
    "# Load testing data\n",
    "if data_mode == 'predicted_trk':\n",
    "    from dataloaders.pred_trkvec_masked import TrkDataset\n",
    "    test_dir1 = '/ssd2/tingting/HFMLNewFiles-old-parsed/nontrigger/0'\n",
    "    test_dir2 = '/ssd2/tingting/HFMLNewFiles-old-parsed/trigger/1'\n",
    "    test_dataset = TrkDataset(input_dir=test_dir1, n_input_dir=2, input_dir2=test_dir2,  \n",
    "        add_geo_features=config['data']['add_geo_features'], add_radius=config['data']['add_radius'],\n",
    "        file_start_index1=700000, file_end_index1=700000+test_samples1, file_start_index2=700000, file_end_index2=700000+test_samples2)\n",
    "elif data_mode == 'gt_track':\n",
    "    from dataloaders.gt_trkvec_masked import TrkDataset\n",
    "    test_dir1 = '/ssd2/tingting/HFMLNewFiles-old-parsed/nontrigger/0'\n",
    "    test_dir2 = '/ssd2/tingting/HFMLNewFiles-old-parsed/trigger/1'\n",
    "    test_dataset = TrkDataset(input_dir=test_dir1, n_input_dir=2, input_dir2=test_dir2, \n",
    "        add_geo_features=config['data']['add_geo_features'], use_radius=config['data']['use_radius'],\n",
    "        file_start_index1=700000, file_end_index1=700000+test_samples1, file_start_index2=700000, file_end_index2=700000+test_samples2)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    import torch\n",
    "    track_list, trigger_list, lengths_list = [], [], []\n",
    "    for (_track, _length, _trigger) in batch:\n",
    "        track_list.append(_track)\n",
    "        lengths_list.append(_length)\n",
    "        trigger_list.append(_trigger)\n",
    "\n",
    "    trigger_list = torch.tensor(trigger_list, dtype=torch.int64)\n",
    "    length_list = torch.tensor(lengths_list, dtype=torch.int64)\n",
    "    track_list = pad_sequence(track_list, batch_first=True, padding_value=0)\n",
    "\n",
    "    return track_list, length_list, trigger_list\n",
    "    \n",
    "collate_fn = collate_batch\n",
    "\n",
    "loader_args = dict(batch_size=batch_size, collate_fn=collate_fn,\n",
    "                num_workers=32)\n",
    "train_sampler, valid_sampler, test_sampler = None, None, None\n",
    "test_data_loader = (DataLoader(test_dataset, sampler=test_sampler, **loader_args)\n",
    "                            if test_dataset is not None else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigger: 100 Non-Trigger: 99900\n",
      "{'prec': 0.0048565791470632875, 'recall': 0.96, 'acc': 0.80325, 'F1': 0.009664267378064128, 'auroc': 0.9362285285285286}\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "\n",
    "preds = []\n",
    "labels = []\n",
    "preds_prob = []\n",
    "count = 0\n",
    "num_insts = 0\n",
    "    \n",
    "for batch in test_data_loader:\n",
    "    count += 1\n",
    "    tracks, n_tracks, trig = batch\n",
    "    tracks = tracks.to(DEVICE, torch.float)\n",
    "    n_tracks = n_tracks.to(DEVICE)\n",
    "    trig = (trig.to(DEVICE) == 1).long()\n",
    "    batch_size = tracks.shape[0]\n",
    "\n",
    "    mask = torch.zeros((tracks.shape[0], tracks.shape[1]))\n",
    "    for i, n_track in enumerate(n_tracks):\n",
    "        mask[i, :n_track] = 1\n",
    "    mask = mask.to(DEVICE)\n",
    "    # mask = mask[:, permutation]\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    num_insts += batch_size\n",
    "    labels.append(trig.long().cpu().numpy())\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        pred_labels = model(tracks, mask)\n",
    "        pred = pred_labels.max(dim=1)[1]\n",
    "        preds.append((pred).cpu().data.numpy())\n",
    "        preds_prob.extend(nn.Softmax(dim=1)(pred_labels)[:, 1].detach().cpu().numpy().flatten())\n",
    "        loss = F.nll_loss(pred_labels, trig)\n",
    "    test_loss += loss.item() * batch_size\n",
    "\n",
    "\n",
    "labels = np.hstack(labels)\n",
    "preds = np.hstack(preds)\n",
    "preds_prob = np.hstack(preds_prob)\n",
    "\n",
    "print(f'Trigger: {sum(labels==1)} Non-Trigger: {sum(labels==0)}')\n",
    "\n",
    "result = {'prec': metrics.precision_score(labels, preds>0),\n",
    "            'recall': metrics.recall_score(labels, preds>0),\n",
    "            'acc': metrics.accuracy_score(labels, preds>0),\n",
    "            'F1': metrics.f1_score(labels, preds>0),\n",
    "            'auroc': metrics.roc_auc_score(labels, preds_prob)}\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 0.1% Trigger Events \t drop_rate: 90.0% \t efficiency: 78.0% \t purity: 0.78%\n",
      "Input 0.1% Trigger Events \t drop_rate: 95.0% \t efficiency: 50.0% \t purity: 1.0%\n",
      "Input 0.1% Trigger Events \t drop_rate: 99.0% \t efficiency: 17.0% \t purity: 1.7%\n",
      "Input 0.1% Trigger Events \t drop_rate: 99.33% \t efficiency: 11.0% \t purity: 1.65%\n"
     ]
    }
   ],
   "source": [
    "def check_efficiency_and_purity(drop_rate, preds_prob, labels):\n",
    "    threshold = np.quantile(preds_prob, drop_rate)\n",
    "    predictions = (preds_prob > threshold)\n",
    "    cm = metrics.confusion_matrix(predictions, labels)\n",
    "    tp = cm[1][1]\n",
    "    tn = cm[0][0]\n",
    "    fn = cm[0][1]\n",
    "    fp = cm[1][0]\n",
    "    # print(cm)\n",
    "    efficiency = tp / (tp + fn)\n",
    "    purity = tp / (tp + fp)\n",
    "    print(f'Input {np.round(100*test_samples2/(test_samples1+test_samples2), 2)}% Trigger Events \\t drop_rate: {np.round(100*drop_rate,2)}% \\t efficiency: {np.round(100*efficiency, 2)}% \\t purity: {np.round(100*purity, 2)}%')\n",
    "\n",
    "check_efficiency_and_purity(0.9, preds_prob, labels)\n",
    "check_efficiency_and_purity(0.95, preds_prob, labels)\n",
    "check_efficiency_and_purity(0.99, preds_prob, labels)\n",
    "check_efficiency_and_purity(1-1/150, preds_prob, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03ca6a50b752edba9a34d5783ddc5b2d6b7cf0ebaad7b34aec90b5ffba5acaaf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
