{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a14a8582-d53d-4deb-b4b4-82ca99a375bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /home1/giorgian/projects/trigger-detection-pipeline/sPHENIX/tracking-GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd3c1658",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dataclasses import replace\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import os.path\n",
    "import sys\n",
    "import logging\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from numpy.linalg import inv\n",
    "import sklearn.metrics as metrics\n",
    "from datasets import get_data_loaders\n",
    "from tqdm.notebook import tqdm\n",
    "import glob\n",
    "from torch_geometric.data import Data\n",
    "import dataclasses\n",
    "from disjoint_set import DisjointSet\n",
    "from typing import Union\n",
    "from numpy.linalg import inv\n",
    "from scipy.stats import mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f738b3f4-78a5-4735-b722-2848d5b3421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class EventInfo:\n",
    "    n_pixels: Union[np.ndarray, torch.Tensor]\n",
    "    energy: Union[np.ndarray, torch.Tensor]\n",
    "    momentum: Union[np.ndarray, torch.Tensor]\n",
    "    interaction_point: Union[np.ndarray, torch.Tensor]\n",
    "    trigger: Union[bool, torch.Tensor]\n",
    "    has_trigger_pair: Union[bool, torch.Tensor]\n",
    "    track_origin: Union[np.ndarray, torch.Tensor]\n",
    "    trigger_node: Union[np.ndarray, torch.Tensor]\n",
    "    particle_id: Union[np.ndarray, torch.Tensor]\n",
    "    particle_type: Union[np.ndarray, torch.Tensor]\n",
    "    parent_particle_type: Union[np.ndarray, torch.Tensor]\n",
    "    track_hits: Union[np.ndarray, torch.Tensor]\n",
    "    track_n_hits: Union[np.ndarray, torch.Tensor]\n",
    "\n",
    "\n",
    "\n",
    "def get_tracks(edge_index):\n",
    "    # Get connected components\n",
    "    ds = DisjointSet()\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        ds.union(edge_index[0, i], edge_index[1, i])\n",
    "\n",
    "    return tuple(list(x) for x in ds.itersets())\n",
    "\n",
    "def load_graph(filename, min_edge_probability, intt_required=False):\n",
    "    layers = [(0,), (1,), (2,), (3,4), (5,6)]\n",
    "    with np.load(filename, allow_pickle=True) as f:\n",
    "        model_edge_probability = f['model_edge_probability']\n",
    "        edge_index = f['edge_index'][:, model_edge_probability >= min_edge_probability]\n",
    "        tracks = get_tracks(edge_index)\n",
    "        if intt_required:\n",
    "            tracks = [track for track in tracks if np.any(f['layer_id'][track] >= 3)]\n",
    "\n",
    "        track_hits = np.zeros((len(tracks), 3*len(layers)))\n",
    "        n_pixels = np.zeros((len(tracks), len(layers)))\n",
    "        energy = np.zeros(len(tracks))\n",
    "        momentum = np.zeros((len(tracks), 3))\n",
    "        track_origin = np.zeros((len(tracks), 3))\n",
    "        trigger_node = np.zeros(len(tracks))\n",
    "        particle_id = np.zeros(len(tracks))\n",
    "        particle_type = np.zeros(len(tracks))\n",
    "        parent_particle_type = np.zeros(len(tracks))\n",
    "        track_n_hits = np.zeros((len(tracks), len(layers)))\n",
    "\n",
    "        for i, track in enumerate(tracks):\n",
    "            layer_id = f['layer_id'][track]\n",
    "            hit_n_pixels = f['n_pixels'][track]\n",
    "            hits = f['hit_cartesian'][track]\n",
    "\n",
    "            # Calculate per-layer information\n",
    "            for j, layer in enumerate(layers):\n",
    "                mask = np.isin(layer_id, layer)\n",
    "                weighted_hits = hit_n_pixels[mask, None] * hits[mask]\n",
    "                d = np.sum(hit_n_pixels[mask])\n",
    "\n",
    "                track_hits[i, 3*j:3*(j+1)] = np.sum(weighted_hits, axis=0)/(d + (d == 0))\n",
    "                n_pixels[i, j] = d\n",
    "                track_n_hits[i, j] = np.sum(mask)\n",
    "            \n",
    "            # Find the GT particle that this track is assigned to\n",
    "            pids = f['particle_id'][track]\n",
    "            particle_id[i] = mode(pids, axis=0, keepdims=False).mode\n",
    "            if np.isnan(particle_id[i]):\n",
    "                index = track[np.where(np.isnan(pids))[0][0]]\n",
    "            else:\n",
    "                index = track[np.where(pids == particle_id[i])[0][0]]\n",
    "\n",
    "            energy[i] = f['energy'][index]\n",
    "            momentum[i] = f['momentum'][index]\n",
    "            track_origin[i] = f['track_origin'][index]\n",
    "            trigger_node[i] = f['trigger_node'][index]\n",
    "            particle_type[i] = f['particle_type'][index]\n",
    "            parent_particle_type[i] = f['parent_particle_type'][index]\n",
    "\n",
    "        return EventInfo(\n",
    "                n_pixels=n_pixels,\n",
    "                energy=energy,\n",
    "                momentum=momentum,\n",
    "                interaction_point=f['interaction_point'],\n",
    "                trigger=f['trigger'],\n",
    "                has_trigger_pair=f['has_trigger_pair'],\n",
    "                track_origin=track_origin,\n",
    "                trigger_node=trigger_node,\n",
    "                particle_id=particle_id,\n",
    "                particle_type=particle_type,\n",
    "                parent_particle_type=parent_particle_type,\n",
    "                track_hits=track_hits,\n",
    "                track_n_hits=track_n_hits\n",
    "        )\n",
    "\n",
    "def get_track_endpoints(hits, good_layers):\n",
    "    # Assumption: all tracks have at least 1 hit\n",
    "    # If it has one hit, first_hit == last_hit for that track\n",
    "    # hits shape: (n_tracks, 5, 3)\n",
    "    # good_layers shape: (n_tracks, 5)\n",
    "    min_indices = good_layers * np.arange(5) + (1 - good_layers) * np.arange(5, 10)\n",
    "    indices = np.expand_dims(np.argmin(min_indices, axis=-1), -1)\n",
    "    indices = np.expand_dims(indices, axis=-2)\n",
    "    first_hits = np.take_along_axis(hits, indices, axis=-2)\n",
    "    max_indices = good_layers * np.arange(5, 10) + (1 - good_layers) * np.arange(5)\n",
    "    indices = np.expand_dims(np.argmax(max_indices, axis=-1), -1)\n",
    "    indices = np.expand_dims(indices, axis=-2)\n",
    "    last_hits = np.take_along_axis(hits, indices, axis=-2)\n",
    "    return first_hits.squeeze(1), last_hits.squeeze(1)\n",
    "\n",
    "def get_predicted_pz(track_hits, good_layers, radius):\n",
    "    hits = track_hits.reshape(-1, 5, 3)\n",
    "    first_hit, last_hit = get_track_endpoints(hits, good_layers)\n",
    "    dz = (last_hit[:, -1] - first_hit[:, -1])/100\n",
    "    chord2 = ((last_hit[:, 0] - first_hit[:, 0]) ** 2 + (last_hit[:, 1] - first_hit[:, 1]) ** 2) / 10000\n",
    "    r2 = 2*radius**2\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        dtheta = np.arccos((r2 - chord2) / (r2 + (r2 == 0)))\n",
    "    dtheta += (dtheta == 0)\n",
    "    return np.nan_to_num(dz / dtheta)\n",
    "\n",
    "def matmul_3D(A, B):\n",
    "    return np.einsum('lij,ljk->lik', A, B)\n",
    "\n",
    "\n",
    "def get_approximate_radii(track_hits, good_layers, n_layers):\n",
    "    x_indices = [3*j for j in range(5)]\n",
    "    y_indices = [3*j+1 for j in range(5)]\n",
    "    r = np.zeros(track_hits.shape[0])\n",
    "    centers = np.zeros((track_hits.shape[0], 2))\n",
    "    for n_layer in range(3, 5 + 1):\n",
    "        complete_tracks = track_hits[n_layers == n_layer]\n",
    "        hit_indices = good_layers[n_layers == n_layer]\n",
    "        if complete_tracks.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        A = np.ones((complete_tracks.shape[0], n_layer, 3))\n",
    "        x_values = complete_tracks[:, x_indices]\n",
    "        x_values = x_values[hit_indices].reshape(complete_tracks.shape[0], n_layer)\n",
    "\n",
    "        y_values = complete_tracks[:, y_indices]\n",
    "        y_values = y_values[hit_indices].reshape(complete_tracks.shape[0], n_layer)\n",
    "        A[:, :, 0] = x_values\n",
    "        A[:, :, 1] = y_values\n",
    "\n",
    "        y = - x_values**2 - y_values**2\n",
    "        y = y.reshape((y.shape[0], y.shape[1], 1))\n",
    "        AT = np.transpose(A, axes=(0, 2, 1))\n",
    "        c = matmul_3D(matmul_3D(inv(matmul_3D(AT, A)), AT), y)[..., 0]\n",
    "        r[n_layers == n_layer] = np.sqrt(c[:, 0]**2 + c[:, 1]**2 - 4*c[:, 2])/200\n",
    "        centers[n_layers == n_layer] = np.stack([-c[:, 0]/2, -c[:, 1]/2], axis=-1)\n",
    "\n",
    "    #test = get_approximate_radius(track_hits, n_layers == 5)\n",
    "    #assert np.allclose(test, r[n_layers == 5])\n",
    "\n",
    "    return r, centers\n",
    "\n",
    "def get_length(start, end):\n",
    "    return np.sqrt(np.sum((start - end)**2, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ae0de67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ceb525f9ed248809434df4930d69794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1082962 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Loop over each file\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m tqdm(all_files):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Load the graph data\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     event_info \u001b[38;5;241m=\u001b[39m load_graph(\n\u001b[1;32m     17\u001b[0m         filename,\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m     19\u001b[0m         intt_required\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     )   \n\u001b[1;32m     22\u001b[0m     good_layers \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39many(event_info\u001b[38;5;241m.\u001b[39mtrack_hits\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m     n_layers \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(good_layers, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m, in \u001b[0;36mload_graph\u001b[0;34m(filename, min_edge_probability, intt_required)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_graph\u001b[39m(filename, min_edge_probability, intt_required\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     28\u001b[0m     layers \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m0\u001b[39m,), (\u001b[38;5;241m1\u001b[39m,), (\u001b[38;5;241m2\u001b[39m,), (\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m), (\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m6\u001b[39m)]\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39mload(filename, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     30\u001b[0m         model_edge_probability \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_edge_probability\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     31\u001b[0m         edge_index \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m][:, model_edge_probability \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_edge_probability]\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:455\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 455\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mfspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    456\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trigger_files = glob.glob('/ssd2/giorgian/tracks-data-mixed-2/trigger/1/*.npz')\n",
    "nontrigger_files = glob.glob('/ssd2/giorgian/tracks-data-mixed-2/nontrigger/1/*.npz')\n",
    "trigger_output_dir = '/disks/disk1/giorgian/beautyllm-pileup/trigger/'\n",
    "nontrigger_output_dir = '/disks/disk1/giorgian/beautyllm-pileup/nontrigger/'\n",
    "\n",
    "output_dirs = (trigger_output_dir, nontrigger_output_dir)\n",
    "\n",
    "for output_dir in output_dirs:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "all_files = trigger_files + nontrigger_files\n",
    "cylindrical_features_scale=np.array([3, 1, 3])\n",
    "# Loop over each file\n",
    "for filename in tqdm(all_files):\n",
    "    # Load the graph data\n",
    "    event_info = load_graph(\n",
    "        filename,\n",
    "        0.5,\n",
    "        intt_required=True\n",
    "    )   \n",
    "\n",
    "    good_layers = np.any(event_info.track_hits.reshape(-1, 5, 3) != 0, axis=-1)\n",
    "    n_layers = np.sum(good_layers, axis=-1)\n",
    "    radii, centers = get_approximate_radii(event_info.track_hits, good_layers, n_layers)\n",
    "\n",
    "    p_z = get_predicted_pz(event_info.track_hits,\n",
    "        good_layers, \n",
    "        radii\n",
    "    )\n",
    "\n",
    "\n",
    "   \n",
    "    if 'event1' in filename:\n",
    "        output_file = os.path.join(trigger_output_dir, os.path.basename(filename))\n",
    "    else:\n",
    "        output_file = os.path.join(nontrigger_output_dir, os.path.basename(filename))\n",
    "    output_file = output_file.replace('.npz', '.txt')\n",
    "\n",
    "    with open(output_file, 'w') as fout:\n",
    "        tracks = event_info.track_hits\n",
    "        #print(f'Here is a particle collision event with {len(tracks)} tracks.', file=fout)\n",
    "       # print(f'The collision vertex is {tuple(pred_ip.tolist())}.', file=fout)\n",
    "        #print(f'{radii=} {p_z=} {centers=} {tracks=}')\n",
    "        #print(f'{radii.shape=} {p_z.shape=} {centers.shape=} {tracks.shape=}')\n",
    "        for i, ti in enumerate(np.random.permutation(tracks.shape[0])):\n",
    "            print(f'Track number {i+1} has a transverse momentum of {radii[ti]}, a parallel momentum of {p_z[ti]}, a center of {tuple(centers[ti].tolist())} and a trajectory of {tuple(tracks[ti].tolist())} as the particle flew through the detector.', file=fout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503a8d6a-5c7c-4acb-af79-f3aa0daec915",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
