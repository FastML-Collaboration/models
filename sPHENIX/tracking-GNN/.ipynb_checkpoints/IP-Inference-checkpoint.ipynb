{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd3c1658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import replace\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import os.path\n",
    "import sys\n",
    "import logging\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from models.garnet_ip import GNNGraphClassifier\n",
    "from numpy.linalg import inv\n",
    "import sklearn.metrics as metrics\n",
    "from datasets import get_data_loaders\n",
    "from tqdm.notebook import tqdm\n",
    "import glob\n",
    "from datasets.hit_graph_trigger_pileup import load_graph\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a9e077",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb5ddd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../trigger_results/agnn/agnn-lr9.77987556971304e-05-b64-d16-PReLU-gi1-ln-True-n500000/experiment_2024-09-05_16:06:22/checkpoints/model_checkpoint_011.pth.tar\n",
      "Successfully reloaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_153961/2076833749.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_file, map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "model_result_folder = '../trigger_results/agnn/agnn-lr9.77987556971304e-05-b64-d16-PReLU-gi1-ln-True-n500000/experiment_2024-09-05_16:06:22/'\n",
    "config_file = model_result_folder + '/config.pkl'\n",
    "config = pickle.load(open(config_file, 'rb'))\n",
    "data_config = config.get('data')\n",
    "dphi_max, dz_max = data_config['phi_slope_max'], data_config['z0_max']\n",
    "\n",
    "model_config = config.get('model', {})\n",
    "model_config.pop('loss_func')\n",
    "model_config.pop('name')\n",
    "model = GNNGraphClassifier(**model_config).to(DEVICE)\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer=None):\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    if optimizer != None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        return model, optimizer\n",
    "    return model\n",
    "\n",
    "# load_checkpoint\n",
    "checkpoint_dir = os.path.join(model_result_folder, 'checkpoints')\n",
    "checkpoint_file = sorted([os.path.join(checkpoint_dir, f) for f in os.listdir(checkpoint_dir) if f.startswith('model_checkpoint')])\n",
    "checkpoint_file = checkpoint_file[-1]\n",
    "print(checkpoint_file)\n",
    "model = load_checkpoint(checkpoint_file, model)\n",
    "print('Successfully reloaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac512a34-fa5b-4532-8784-922f4ab26669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_circles_to_particles(hits, particle_ids):\n",
    "    \"\"\"\n",
    "    Fits a circle to each particle's hits using only the x and y coordinates.\n",
    "\n",
    "    Parameters:\n",
    "    hits (numpy.ndarray): An array of shape (n_hits, 3) containing hit coordinates (x, y, z).\n",
    "    particle_ids (numpy.ndarray): An array of shape (n_hits,) containing particle IDs corresponding to each hit.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary of centers keyed by particle ID.\n",
    "    dict: A dictionary of radii keyed by particle ID.\n",
    "    \"\"\"\n",
    "    unique_pids = np.unique(particle_ids)\n",
    "    centers = {}\n",
    "    radii = {}\n",
    "\n",
    "    for pid in unique_pids:\n",
    "        # Extract indices of hits corresponding to the current particle ID\n",
    "        indices = np.where(particle_ids == pid)[0]\n",
    "        # Extract x and y coordinates; ignore z\n",
    "        x = hits[indices, 0]\n",
    "        y = hits[indices, 1]\n",
    "\n",
    "        if len(x) < 3:\n",
    "            # Cannot fit a circle with less than 3 points\n",
    "            continue\n",
    "\n",
    "        # Set up the linear system for circle fitting in the x-y plane\n",
    "        D = np.column_stack((x, y, np.ones_like(x)))\n",
    "        RHS = x**2 + y**2\n",
    "\n",
    "        # Solve D * params = -RHS to find the circle parameters\n",
    "        params, residuals, rank, s = np.linalg.lstsq(D, -RHS, rcond=None)\n",
    "        A, B, C = params\n",
    "        # Calculate the circle center coordinates\n",
    "        x0 = -A / 2\n",
    "        y0 = -B / 2\n",
    "        # Calculate the radius squared\n",
    "        R_squared = x0**2 + y0**2 - C\n",
    "\n",
    "        if R_squared < 0:\n",
    "            R = np.nan  # Invalid radius (due to numerical issues)\n",
    "        else:\n",
    "            R = np.sqrt(R_squared)\n",
    "\n",
    "        centers[pid] = (x0, y0)\n",
    "        radii[pid] = R\n",
    "\n",
    "    return centers, radii\n",
    "\n",
    "def create_particle_tracks(hits, particle_ids, raw_layers):\n",
    "    \"\"\"\n",
    "    Creates a track for each particle by pooling hits per layer and computing the mean position.\n",
    "\n",
    "    Parameters:\n",
    "    hits (numpy.ndarray): An array of shape (n_hits, 3) containing hit coordinates (x, y, z).\n",
    "    particle_ids (numpy.ndarray): An array of shape (n_hits,) containing particle IDs corresponding to each hit.\n",
    "    raw_layers (numpy.ndarray): An array of shape (n_hits,) containing raw layer indices for each hit.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary mapping particle IDs to their track arrays of shape (15,).\n",
    "    \"\"\"\n",
    "    # Mapping from raw layers to true layers\n",
    "    raw_layer_to_layer = {0: 0, 1: 1, 2: 2, 3: 3, 4: 3, 5: 4, 6: 4}\n",
    "    # Vectorize the mapping for efficiency\n",
    "    vectorized_mapping = np.vectorize(raw_layer_to_layer.get)\n",
    "    # Map raw layers to true layers\n",
    "    layers = vectorized_mapping(raw_layers)\n",
    "    \n",
    "    # Unique particle IDs\n",
    "    unique_pids = np.unique(particle_ids)\n",
    "    tracks = {}\n",
    "\n",
    "    for pid in unique_pids:\n",
    "        # Indices of hits corresponding to the current particle\n",
    "        indices = np.where(particle_ids == pid)[0]\n",
    "        particle_hits = hits[indices]  # (n_hits_particle, 3)\n",
    "        particle_layers = layers[indices]  # (n_hits_particle,)\n",
    "\n",
    "        # Initialize track array with NaNs for missing layers\n",
    "        track = np.full((5, 3), np.nan)\n",
    "\n",
    "        for layer in range(5):\n",
    "            # Indices of hits in the current layer\n",
    "            layer_indices = np.where(particle_layers == layer)[0]\n",
    "            if len(layer_indices) > 0:\n",
    "                # Hits in the current layer\n",
    "                hits_in_layer = particle_hits[layer_indices]\n",
    "                # Mean position of hits in the current layer\n",
    "                mean_position = np.mean(hits_in_layer, axis=0)\n",
    "                # Store mean position in the track\n",
    "                track[layer] = mean_position\n",
    "            # If no hits in the layer, leave NaNs\n",
    "\n",
    "        # Flatten the track to shape (15,)\n",
    "        track_flat = track.flatten()\n",
    "        # Store the track in the dictionary\n",
    "        tracks[pid] = track_flat\n",
    "\n",
    "    return tracks\n",
    "\n",
    "def get_track_endpoints(hits, good_layers):\n",
    "    # Assumption: all tracks have at least 1 hit\n",
    "    # If it has one hit, first_hit == last_hit for that track\n",
    "    # hits shape: (n_tracks, 5, 3)\n",
    "    # good_layers shape: (n_tracks, 5)\n",
    "    min_indices = good_layers * np.arange(5) + (1 - good_layers) * np.arange(5, 10)\n",
    "    indices = np.expand_dims(np.argmin(min_indices, axis=-1), -1)\n",
    "    indices = np.expand_dims(indices, axis=-2)\n",
    "    first_hits = np.take_along_axis(hits, indices, axis=-2)\n",
    "    max_indices = good_layers * np.arange(5, 10) + (1 - good_layers) * np.arange(5)\n",
    "    indices = np.expand_dims(np.argmax(max_indices, axis=-1), -1)\n",
    "    indices = np.expand_dims(indices, axis=-2)\n",
    "    last_hits = np.take_along_axis(hits, indices, axis=-2)\n",
    "    return first_hits.squeeze(1), last_hits.squeeze(1)\n",
    "\n",
    "def get_predicted_pz(track_hits, good_layers, radius):\n",
    "    hits = track_hits.reshape(-1, 5, 3)\n",
    "    first_hit, last_hit = get_track_endpoints(hits, good_layers)\n",
    "    dz = (last_hit[:, -1] - first_hit[:, -1])/100\n",
    "    chord2 = ((last_hit[:, 0] - first_hit[:, 0]) ** 2 + (last_hit[:, 1] - first_hit[:, 1]) ** 2) / 10000\n",
    "    r2 = 2*radius**2\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        dtheta = np.arccos((r2 - chord2) / (r2 + (r2 == 0)))\n",
    "    dtheta += (dtheta == 0)\n",
    "    return np.nan_to_num(dz / dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ae0de67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69228eea423d4fd6a5ea468d3cf1c7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1039700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/largehome/giorgian/projects/trigger-detection-pipeline/sPHENIX/tracking-GNN/models/garnet_ip.py:127: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789560443/work/torch/csrc/tensor/python_tensor.cpp:78.)\n",
      "  temp = torch.cuda.FloatTensor(x.shape[0]).fill_(1)\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trigger_files = glob.glob('/secondssd/giorgian/hits-data-august-2024/trigger/1/*.npz')\n",
    "nontrigger_files = glob.glob('/secondssd/giorgian/hits-data-august-2024/trigger/1/*.npz')\n",
    "trigger_output_dir = '/home/giorgian/beautyllm/trigger/'\n",
    "nontrigger_output_dir = '/home/giorgian/beautyllm/nontrigger/'\n",
    "\n",
    "output_dirs = (trigger_output_dir, nontrigger_output_dir)\n",
    "\n",
    "for output_dir in output_dirs:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "all_files = trigger_files + nontrigger_files\n",
    "cylindrical_features_scale=np.array([3, 1, 3])\n",
    "# Loop over each file\n",
    "for filename in tqdm(all_files):\n",
    "    # Load the graph data\n",
    "    x, edge_index, y, event_info = load_graph(\n",
    "        filename,\n",
    "        cylindrical_features_scale,\n",
    "        0,\n",
    "        0,\n",
    "        use_intt=True,\n",
    "        construct_edges=False,\n",
    "        drop_l1=False,\n",
    "        drop_l2=False,\n",
    "        drop_l3=False,\n",
    "        add_global_node=False\n",
    "    )   \n",
    "    \n",
    "    batch = np.zeros(x.shape[0])\n",
    "\n",
    "    # Create a Data object for PyTorch Geometric\n",
    "    data = Data(\n",
    "        x=torch.tensor(x, dtype=torch.float),\n",
    "        edge_index=torch.tensor(edge_index, dtype=torch.long),\n",
    "        batch=torch.tensor(batch, dtype=torch.long)\n",
    "    )\n",
    "\n",
    "    # Send data to the device\n",
    "    data = data.to(DEVICE)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        pred = model(data).detach().cpu().numpy()\n",
    "\n",
    "    pred_ip = pred[0][:2]\n",
    "\n",
    "\n",
    "\n",
    "    f = np.load(filename)\n",
    "    pids = np.unique(f['particle_id'])\n",
    "    tracks = create_particle_tracks(f['hit_cartesian'], f['particle_id'], f['layer_id'])\n",
    "    centers, radii = fit_circles_to_particles(f['hit_cartesian'], f['particle_id'])\n",
    "    \n",
    "    tracks = np.stack([tracks[pid] for pid in pids], axis=0)\n",
    "    tracks[np.isnan(tracks)] = 0\n",
    "    \n",
    "    centers = np.stack([centers[pid] if pid in centers else np.array([0, 0]) for pid in pids], axis=0)\n",
    "    radii = np.array([radii[pid] if pid in radii else 0 for pid in pids])\n",
    "    good_layers = np.any(tracks.reshape(-1, 5, 3), axis=-1)\n",
    "    p_z = get_predicted_pz(tracks, good_layers, radii)\n",
    "\n",
    "    if 'event1' in filename:\n",
    "        output_file = os.path.join(trigger_output_dir, os.path.basename(filename))\n",
    "    else:\n",
    "        output_file = os.path.join(nontrigger_output_dir, os.path.basename(filename))\n",
    "    output_file = output_file.replace('.npz', '.txt')\n",
    "\n",
    "    with open(output_file, 'w') as fout:\n",
    "        print(f'Here is a particle collision event with {len(tracks)} tracks.', file=fout)\n",
    "        print(f'The collision vertex is {tuple(pred_ip.tolist())}.', file=fout)\n",
    "    \n",
    "        for i, ti in enumerate(np.random.permutation(tracks.shape[0])):\n",
    "            print(f'Track number {i+1} has a transverse momentum of {radii[ti]}, a parallel momentum of {p_z[ti]}, a center of {tuple(centers[ti].tolist())} and a trajectory of {tuple(tracks[ti].tolist())} as the particle flew through the detector.', file=fout)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
